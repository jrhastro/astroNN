{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Investigating using Convolutional Networks on Weak Lensing data\n",
    "=============\n",
    "\n",
    "Adapted from 4_conv_WL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "plt.rcParams['image.cmap'] = 'viridis'\n",
    "plt.rcParams['image.interpolation'] = 'none'\n",
    "%matplotlib inline\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reorganize the code a bit by putting the function definitions first.\n",
    "def rebin(a, shape):\n",
    "    sh = shape[0],a.shape[0]//shape[0],shape[1],a.shape[1]//shape[1]\n",
    "    return a.reshape(sh).mean(-1).mean(1)\n",
    "\n",
    "def getFITS(imagename):\n",
    "    filename = whereami + '/' + path + imagename\n",
    "    f = fits.open(filename)\n",
    "    dataout = f[0].data\n",
    "    \n",
    "    return dataout\n",
    "\n",
    "def read_WL(path,display=None):\n",
    "    # this is a version to look at sigma8\n",
    "    labels=['750', '850']\n",
    "    imgs = np.zeros([2048/degrade, 2048/degrade, nct, len(labels)])\n",
    "    for j, label in enumerate(labels):\n",
    "        for i in range(nct):\n",
    "            filename = whereami + '/' + path + 'smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.'+label+'_4096xy_000'+ np.str(i+1) +'r_0029p_0100z_og.gre.fit'\n",
    "            if display: print(\"i: %d  j: %d  name: %s\" % (i, j, 'smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.'+label+'_4096xy_000'+ np.str(i+1) +'r_0029p_0100z_og.gre.fit'))\n",
    "            f = fits.open(filename)\n",
    "            imgs[:,:,i,j]=rebin(f[0].data, [2048/degrade, 2048/degrade])\n",
    "            \n",
    "    return imgs, labels\n",
    "\n",
    "def slice_data(data, labels, exp_cut, exp_nshift):\n",
    "    labels=['750', '850']\n",
    "    # how many panels across\n",
    "    npanelx = 2**exp_cut\n",
    "    # and how big are they?\n",
    "    panelw = 2048/(degrade*npanelx)\n",
    "    # how many shifted panels?\n",
    "    nshift = 2**exp_nshift -1\n",
    "    # and what are the shifts?\n",
    "    shiftw =  panelw/2**exp_nshift\n",
    "    # with 4 rotations, and 2 shifts, we have\n",
    "    imgs = np.zeros([panelw, panelw, nct,(npanelx**2 +(npanelx-1)**2*nshift**2)*8, len(labels)])\n",
    "    # let's figure out where the centers are, and save that data\n",
    "    x_centers = np.zeros([nct,(npanelx**2 +(npanelx-1)**2*nshift**2)*8, len(labels)])\n",
    "    y_centers = np.zeros([nct,(npanelx**2 +(npanelx-1)**2*nshift**2)*8, len(labels)])\n",
    "    for j, label in enumerate(labels):\n",
    "        for i in range(nct):\n",
    "            q=0\n",
    "            for k in range(npanelx):\n",
    "                for l in range(npanelx):\n",
    "                    for r in range(4):\n",
    "                        imgs[:,:,i,q,j] = np.rot90(data[panelw*k:panelw*(k+1),panelw*l:panelw*(l+1),i, j], r)\n",
    "                        x_centers[i,q,j] = (panelw*k+panelw*(k+1))/2.\n",
    "                        y_centers[i,q,j] = (panelw*l+panelw*(l+1))/2.\n",
    "                        q+=1\n",
    "                        imgs[:,:,i,q,j] = np.fliplr(np.rot90(data[panelw*k:panelw*(k+1),panelw*l:panelw*(l+1),i, j], r))\n",
    "                        x_centers[i,q,j] = (panelw*k+panelw*(k+1))/2.\n",
    "                        y_centers[i,q,j] = (panelw*l+panelw*(l+1))/2.\n",
    "                        q+=1\n",
    "            for k in range(npanelx-1):\n",
    "                for l in range(npanelx-1):\n",
    "                    for m in range(nshift):\n",
    "                        for n in range(nshift):\n",
    "                            for r in range(4):\n",
    "                                imgs[:,:,i,q,j] = np.rot90(data[panelw*k+m*shiftw:panelw*(k+1)+m*shiftw,panelw*l+n*shiftw:panelw*(l+1)+n*shiftw,i, j], r)\n",
    "                                x_centers[i,q,j] = (panelw*k+m*shiftw+panelw*(k+1)+m*shiftw)/2.\n",
    "                                y_centers[i,q,j] = (panelw*l+n*shiftw+panelw*(l+1)+n*shiftw)/2.\n",
    "                                q+=1\n",
    "                                imgs[:,:,i,q,j] = np.fliplr(np.rot90(data[panelw*k+m*shiftw:panelw*(k+1)+m*shiftw,panelw*l+n*shiftw:panelw*(l+1)+n*shiftw,i, j], r))\n",
    "                                x_centers[i,q,j] = (panelw*k+m*shiftw+panelw*(k+1)+m*shiftw)/2.\n",
    "                                y_centers[i,q,j] = (panelw*l+n*shiftw+panelw*(l+1)+n*shiftw)/2.\n",
    "                                q+=1\n",
    "    return imgs, x_centers, y_centers\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the paths to the raw data files\n",
    "whereami = '/home/jhargis'\n",
    "#whereami = '/Users/jhargis'\n",
    "path     = 'Dropbox/astroNN/wl_maps/'\n",
    "#whereami = '/Users/goldston'\n",
    "#whereami = '/Users/jegpeek'\n",
    "#path = 'Documents/Weak_Lensing/kmaps_smoothed/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0001r_0029p_0100z_og.gre.fit\n",
      "i: 1  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0002r_0029p_0100z_og.gre.fit\n",
      "i: 2  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0003r_0029p_0100z_og.gre.fit\n",
      "i: 3  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0004r_0029p_0100z_og.gre.fit\n",
      "i: 4  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0005r_0029p_0100z_og.gre.fit\n",
      "i: 5  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0006r_0029p_0100z_og.gre.fit\n",
      "i: 6  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0007r_0029p_0100z_og.gre.fit\n",
      "i: 7  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0008r_0029p_0100z_og.gre.fit\n",
      "i: 8  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0009r_0029p_0100z_og.gre.fit\n",
      "i: 0  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0001r_0029p_0100z_og.gre.fit\n",
      "i: 1  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0002r_0029p_0100z_og.gre.fit\n",
      "i: 2  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0003r_0029p_0100z_og.gre.fit\n",
      "i: 3  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0004r_0029p_0100z_og.gre.fit\n",
      "i: 4  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0005r_0029p_0100z_og.gre.fit\n",
      "i: 5  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0006r_0029p_0100z_og.gre.fit\n",
      "i: 6  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0007r_0029p_0100z_og.gre.fit\n",
      "i: 7  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0008r_0029p_0100z_og.gre.fit\n",
      "i: 8  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0009r_0029p_0100z_og.gre.fit\n",
      "Data shape : (256, 256, 9, 2)\n",
      "Labels     : ['750', '850']\n"
     ]
    }
   ],
   "source": [
    "# Set (1) the factor by which we want to degrade the original WL maps\n",
    "# and (2) the number of realizations of each universe.\n",
    "#\n",
    "#   The original images are 2048 x 2048, and we degrade them using\n",
    "#   an 8 x 8 sq.pix box, which makes a smaller set of 64 images \n",
    "#   (= 256x256 sq.pix in size).\n",
    "\n",
    "degrade=8\n",
    "nct = 9\n",
    "\n",
    "data, labels = read_WL(path,display=True)\n",
    "\n",
    "print \"Data shape :\",data.shape\n",
    "print \"Labels     :\",labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display an example of the full 2048 x 2048 image\n",
    "#fullimage = getFITS(\"smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0001r_0029p_0100z_og.gre.fit\")\n",
    "#plt.imshow(fullimage, origin=\"lower\")\n",
    "#plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now compare a small section of the original image to the rebinned version\n",
    "#   Becaused we used an 8x8 box, a 256x256 region in the original image\n",
    "#   should correspond to a 32x32 region in the rebinned image\n",
    "#plt.imshow(fullimage[:256,:256],origin=\"lower\")\n",
    "#plt.colorbar()\n",
    "#plt.show()\n",
    "\n",
    "#image_data = data[:32,:32,0,0]\n",
    "#image_data.shape\n",
    "#plt.imshow(image_data, origin='lower')\n",
    "#plt.colorbar()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:9: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:10: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:11: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:13: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:16: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:17: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:18: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:20: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:23: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:24: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:25: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:27: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "# First we slice the data in the following manner:\n",
    "#\n",
    "#   1) Each 256 x 256 image is again split 8 x 8 into 64 images which are 32 x 32 sq. pix in size\n",
    "#   2) Series of 4 rotations, 2 flips, and shifts of 2 pixels in size\n",
    "imgs2, x_centers, y_centers = slice_data(data, labels, 3, 3)\n",
    "img2sh = imgs2.shape\n",
    "\n",
    "# Next, reshape the arrays and take the first 7 realizations as the training data set\n",
    "train_dataset = np.transpose(imgs2[:, :, 0:7, :, :].reshape(img2sh[0], img2sh[1], 7.0*img2sh[3]*2.0), (2, 0, 1))\n",
    "train_xc = x_centers[0:7, :, :].reshape(7.0*img2sh[3]*2.0)\n",
    "train_yc = y_centers[0:7, :, :].reshape(7.0*img2sh[3]*2.0)\n",
    "ones = np.ones([7,img2sh[3], 2] )\n",
    "train_labels = ((np.asarray([0,1])).reshape(1, 1, 2)*ones).reshape(7.0*img2sh[3]*2.0)\n",
    "\n",
    "# The validation set is the 8th realization\n",
    "valid_dataset = np.transpose(imgs2[:, :, 7, :, :].reshape(img2sh[0], img2sh[1], 1.0*img2sh[3]*2.0), (2, 0, 1))\n",
    "valid_xc = x_centers[7, :, :].reshape(1.0*img2sh[3]*2.0)\n",
    "valid_yc = y_centers[7, :, :].reshape(1.0*img2sh[3]*2.0)\n",
    "ones = np.ones([1,img2sh[3], 2] )\n",
    "valid_labels = ((np.asarray([0,1])).reshape(1, 1, 2)*ones).reshape(1.0*img2sh[3]*2.0)\n",
    "\n",
    "# The test data set is the 9th realization\n",
    "test_dataset = np.transpose(imgs2[:, :, 8, :, :].reshape(img2sh[0], img2sh[1], 1.0*img2sh[3]*2.0), (2, 0, 1))\n",
    "test_xc = x_centers[8, :, :].reshape(1.0*img2sh[3]*2.0)\n",
    "test_yc = y_centers[8, :, :].reshape(1.0*img2sh[3]*2.0)\n",
    "ones = np.ones([1,img2sh[3], 2] )\n",
    "test_labels = ((np.asarray([0,1])).reshape(1, 1, 2)*ones).reshape(1.0*img2sh[3]*2.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master images tensor shape: (32, 32, 9, 19720, 2)\n",
      "\n",
      "Train dataset shape: (276080, 32, 32)\n",
      "Train labels shape : (276080,)\n",
      "Test dataset shape : (39440, 32, 32)\n",
      "Valid dataset shape: (39440, 32, 32)\n",
      "TOTAL data sets    :  354960\n"
     ]
    }
   ],
   "source": [
    "print \"Master images tensor shape:\", img2sh\n",
    "print\n",
    "print \"Train dataset shape:\", train_dataset.shape\n",
    "print \"Train labels shape :\", train_labels.shape\n",
    "print \"Test dataset shape :\", test_dataset.shape\n",
    "print \"Valid dataset shape:\", valid_dataset.shape\n",
    "print \"TOTAL data sets    : \", train_dataset.shape[0] + test_dataset.shape[0] + valid_dataset.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd87189fd50>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6EvSjFj0JWlGLPqSNCP/D76+BEMbBYhxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd871928850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.reshape(x_centers, 9*19720*2), np.reshape(y_centers, 9*19720*2), '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"
   },
   "outputs": [],
   "source": [
    "# Just don't even worry about this for now.\n",
    "# Figure it out later.\n",
    "\n",
    "\"\"\"\n",
    "#pickle_file = 'notMNIST.pickle'\n",
    "#pickle_file = '/Users/jegpeek/Documents/WL88.pickle'\n",
    "pickle_file = '/Users/jegpeek/Dropbox/WL_other.pickle'\n",
    "\n",
    "usePickle = True\n",
    "\n",
    "if usePickle:\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "      save = pickle.load(f)\n",
    "      train_dataset = save['train_dataset']\n",
    "      train_labels = save['train_labels']\n",
    "      valid_dataset = save['valid_dataset']\n",
    "      valid_labels = save['valid_labels']\n",
    "      test_dataset = save['test_dataset']\n",
    "      test_labels = save['test_labels']\n",
    "      del save  # hint to help gc free up memory\n",
    "      print('Training set', train_dataset.shape, train_labels.shape)\n",
    "      print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "      print('Test set', test_dataset.shape, test_labels.shape)\n",
    "else:\n",
    "    %run Read_WL.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (276080, 32, 32, 1), (276080, 2))\n",
      "('Validation set', (39440, 32, 32, 1), (39440, 2))\n",
      "('Test set', (39440, 32, 32, 1), (39440, 2))\n"
     ]
    }
   ],
   "source": [
    "# Reformat into a TensorFlow-friendly shape:\n",
    "# - convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "# - labels as float 1-hot encodings.\n",
    "\n",
    "image_size = 32\n",
    "num_labels = 2\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "permutation = np.random.permutation(train_labels.shape[0])\n",
    "train_dataset = train_dataset[permutation,:,:]\n",
    "train_labels = train_labels[permutation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "IZYv70SvvOan"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxiCNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd8719ea8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 9200: 0.741998 (0.004 sec)\n",
      "Minibatch accuracy: 37.5%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16  # 16\n",
    "patch_size = 5    # 5\n",
    "depth = 16        # 16\n",
    "num_hidden = 64   # 64\n",
    "\n",
    "#graph = tf.Graph()\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    # Define the network model\n",
    "    def model(data):\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "   \n",
    "    # Provide summary statistics for an input variable\n",
    "    def variable_summaries(var, name):\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.scalar_summary('mean/' + name, mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n",
    "        tf.scalar_summary('sttdev/' + name, stddev)\n",
    "        tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "        tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "        tf.histogram_summary(name, var)\n",
    "    \n",
    "    # Set the input data placeholders\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Global Step\n",
    "    #global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    #learn_decay = 0.85\n",
    "    #learning_rate = tf.train.exponential_decay(0.005, global_step, 10000, learn_decay, staircase=True)\n",
    "    learning_rate = 0.0005\n",
    "\n",
    "    # Set the layer weights and bias variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal([image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "    #with tf.name_scope('derp'):\n",
    "    #  spl = tf.split(3, 16, layer1_weights)\n",
    "    #  filter_summary = tf.image_summary((spl[0]).name, spl[0], max_images=1)\n",
    "\n",
    "\n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels), name='xentropy_mean')\n",
    "  \n",
    "    # Add the weights and biases to TensorBoard for real-time tracking\n",
    "    with tf.name_scope('layer1'):\n",
    "        # This Variable will hold the state of the weights for the layer\n",
    "        with tf.name_scope(\"weights\"):\n",
    "            variable_summaries(layer1_weights, 'layer1/weights')\n",
    "            with tf.name_scope(\"images\"):\n",
    "                spl = tf.split(3, 16, layer1_weights)\n",
    "                tf.image_summary(\"layer1/weights\", spl[0])\n",
    "        with tf.name_scope(\"biases\"):\n",
    "            variable_summaries(layer1_biases, 'layer1/biases')\n",
    "                \n",
    "    with tf.name_scope('layer2'):\n",
    "        # This Variable will hold the state of the weights for the layer\n",
    "        with tf.name_scope(\"weights\"):\n",
    "            variable_summaries(layer2_weights, 'layer2/weights')\n",
    "            with tf.name_scope(\"images\"):\n",
    "                spl = tf.split(3, 16, layer2_weights)\n",
    "                tf.image_summary(\"layer2/weights\", spl[0])\n",
    "        with tf.name_scope(\"biases\"):\n",
    "            variable_summaries(layer2_biases, 'layer2/biases')\n",
    "\n",
    "    #with tf.name_scope('layer3'):\n",
    "    #    # This Variable will hold the state of the weights for the layer\n",
    "    #    with tf.name_scope(\"weights\"):\n",
    "    #        variable_summaries(layer3_weights, 'layer3/weights')\n",
    "    #        with tf.name_scope(\"images\"):\n",
    "    #            #spl = tf.split(3, 16, layer3_weights)\n",
    "    #            spl = tf.split(1, 64, layer3_weights)\n",
    "    #            tf.image_summary(\"layer3/weights\", spl[0])\n",
    "    #    with tf.name_scope(\"biases\"):\n",
    "    #        variable_summaries(layer3_biases, 'layer3/biases')\n",
    "                \n",
    "    #with tf.name_scope('layer4'):\n",
    "    #    # This Variable will hold the state of the weights for the layer\n",
    "    #    with tf.name_scope(\"weights\"):\n",
    "    #        variable_summaries(layer4_weights, 'layer4/weights')\n",
    "    #        with tf.name_scope(\"images\"):\n",
    "    #            #spl = tf.split(3, 16, layer4_weights)\n",
    "    #            spl = tf.split(1, 2, layer4_weights)\n",
    "    #            tf.image_summary(\"layer4/weights\", spl[0])\n",
    "    #    with tf.name_scope(\"biases\"):\n",
    "    #        variable_summaries(layer4_biases, 'layer4/biases')\n",
    "                \n",
    "    with tf.name_scope('loss'):\n",
    "        # Try to log the loss\n",
    "        tf.scalar_summary(loss.op.name, loss)\n",
    "        \n",
    "    with tf.name_scope('learning_rate'):\n",
    "        tf.scalar_summary('learning_rate', learning_rate)\n",
    "                \n",
    "    # Optimizer and training op\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))\n",
    "\n",
    "    merged = tf.merge_all_summaries()\n",
    "    \n",
    "    # Create a saver for writing training checkpoints.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # Create a session for running Ops on the Graph.\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    # Run the Op to initialize the variables.\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Start the SummaryWriter\n",
    "    summary_writer = tf.train.SummaryWriter(whereami+'/Documents/logs', sess.graph_def)\n",
    "    \n",
    "    # Initialize the looping variables\n",
    "    num_steps = 200001\n",
    "    print_step = 200\n",
    "    summary_step = 200\n",
    "    losses = np.zeros((num_steps-1)/print_step+1)\n",
    "    acc_valid = np.zeros((num_steps-1)/print_step+1)\n",
    "    acc_test = np.zeros((num_steps-1)/print_step+1)\n",
    "    q = 0\n",
    "    p = 0\n",
    "    \n",
    "    # Start the training loop\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        # Log the starting time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Setup the mini-batch\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]        \n",
    "        \n",
    "        # Setup the feed dictionary and run a single step\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        #summary, _, l, predictions = session.run([merged, train_op, loss, train_prediction], feed_dict=feed_dict)\n",
    "        _, l, predictions = sess.run([train_op, loss, train_prediction], feed_dict=feed_dict)\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        # Save and print output\n",
    "        if (step % print_step == 0):\n",
    "            \n",
    "            # Save the losses and accuracy calculations\n",
    "            losses[q] = l\n",
    "            acc_valid[q] = accuracy(valid_prediction.eval(session=sess), valid_labels)/100.0\n",
    "            acc_test[q] = accuracy(test_prediction.eval(session=sess), test_labels)/100.0\n",
    "            \n",
    "            with tf.name_scope('test_accuracy'):\n",
    "                tf.scalar_summary('test_accuracy', acc_test[q])\n",
    "            #summary_writer.add_summary(summary, q)\n",
    "            \n",
    "            # Write the summaries\n",
    "            summary_str = sess.run(merged, feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            \n",
    "            q += 1\n",
    "            plt.plot(np.arange(0,(num_steps-1)/print_step+1), acc_valid, '.', color='b')\n",
    "            plt.plot((-1)*np.arange(0,(num_steps-1)/print_step+1),acc_test, '.', color='g')\n",
    "            plt.ylim([0.45, 0.75])\n",
    "            plt.xlim([-1000, 1000])\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "            print('Minibatch loss at step %d: %f (%.3f sec)' % (step, l, duration))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            #print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(session=sess), valid_labels))\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#permutation = np.random.permutation(train_labels.shape[0])\n",
    "#train_dataset = train_dataset[permutation,:,:]\n",
    "#train_labels = train_labels[permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628"
   },
   "outputs": [],
   "source": [
    "# Run of TensorFlow without the permutations\n",
    "# but with the TensorBoard logging calls\n",
    "num_steps = 400001\n",
    "print_step = 200\n",
    "summary_step = 200\n",
    "losses = np.zeros((num_steps-1)/print_step+1)\n",
    "acc_valid = np.zeros((num_steps-1)/print_step+1)\n",
    "acc_test = np.zeros((num_steps-1)/print_step+1)\n",
    "q = 0\n",
    "p = 0\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    summary_writer = tf.train.SummaryWriter(whereami+'/Documents/logs', session.graph_def)\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]        \n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        summary, _, l, predictions = session.run([merged, optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % print_step == 0):\n",
    "            losses[q] = l\n",
    "            acc_valid[q] = accuracy(valid_prediction.eval(), valid_labels)/100.0\n",
    "            acc_test[q] = accuracy(test_prediction.eval(), test_labels)/100.0\n",
    "            with tf.name_scope('test_accuracy'):\n",
    "                tf.scalar_summary('test_accuracy', acc_test[q])\n",
    "            summary_writer.add_summary(summary, q)\n",
    "            q += 1\n",
    "            plt.plot(np.arange(0,(num_steps-1)/print_step+1), acc_valid, '.', color='b')\n",
    "            plt.plot((-1)*np.arange(0,(num_steps-1)/print_step+1),acc_test, '.', color='g')\n",
    "            plt.ylim([0.45, 0.75])\n",
    "            plt.xlim([-1000, 1000])\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            #print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            #print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "        if (step % summary_step == 0):\n",
    "            print \"got here\"\n",
    "            summary_writer.add_summary(summary, p)\n",
    "            p += 1\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# permutation = np.random.permutation(train_labels.shape[0])\n",
    "#train_dataset = train_dataset[permutation,:,:]\n",
    "#train_labels = train_labels[permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 20001\n",
    "print_step = 200\n",
    "losses = np.zeros((num_steps-1)/print_step+1)\n",
    "acc_valid = np.zeros((num_steps-1)/print_step+1)\n",
    "acc_test = np.zeros((num_steps-1)/print_step+1)\n",
    "q = 0\n",
    "p=0\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % print_step == 0):\n",
    "            losses[q] = l\n",
    "            acc_valid[q] = accuracy(valid_prediction.eval(), valid_labels)/100.0\n",
    "            acc_test[q] = accuracy(test_prediction.eval(), test_labels)/100.0\n",
    "            q += 1\n",
    "            plt.plot(np.arange(0,(num_steps-1)/print_step+1), acc_valid, '.', color='b')\n",
    "            plt.plot((-1)*np.arange(0,(num_steps-1)/print_step+1),acc_test, '.', color='g')\n",
    "            plt.ylim([0.45, 0.65])\n",
    "            plt.xlim([-1000, 1000])\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            #print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            #print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0,(num_steps-1)/print_step+1), acc_valid, '.', color='b')\n",
    "plt.plot((-1)*np.arange(0,(num_steps-1)/print_step+1),acc_test, '.', color='g')\n",
    "plt.ylim([0.45, 0.75])\n",
    "plt.xlim([-1000, 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(acc_test, acc_valid, '.', color='b')\n",
    "#plt.plot((-1)*np.arange(0,(num_steps-1)/print_step+1),losses, '.', color='g')\n",
    "plt.ylim([0.4, 0.7])\n",
    "plt.xlim([0.0, 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Open questions:**\n",
    "+ Why is there so much scatter in the loss function over time?\n",
    "+ Is there structure in the loss function over time?\n",
    "+ If I plot loss vs. accuracy, what do I get? \n",
    "+ Do I really see a difference when I scramble vs. leave in order, and if so, is it because of the way SGD interacts with the two cosmologies?\n",
    "+ will deeper / better networks get us over 65%?\n",
    "+ Why, oh why, are my test and valid data sets so damn well correlated??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph.get_tensor_by_name.im_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KedKkn4EutIK"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klf21gpbAgb-"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
