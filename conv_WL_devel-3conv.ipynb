{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Investigating using Convolutional Networks on Weak Lensing data\n",
    "=============\n",
    "\n",
    "Adapted from 4_conv_WL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "plt.rcParams['image.cmap'] = 'viridis'\n",
    "plt.rcParams['image.interpolation'] = 'none'\n",
    "%matplotlib inline\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reorganize the code a bit by putting the function definitions first.\n",
    "def rebin(a, shape):\n",
    "    sh = shape[0],a.shape[0]//shape[0],shape[1],a.shape[1]//shape[1]\n",
    "    return a.reshape(sh).mean(-1).mean(1)\n",
    "\n",
    "def getFITS(imagename):\n",
    "    filename = whereami + '/' + path + imagename\n",
    "    f = fits.open(filename)\n",
    "    dataout = f[0].data\n",
    "    \n",
    "    return dataout\n",
    "\n",
    "def read_WL(path,display=None):\n",
    "    # this is a version to look at sigma8\n",
    "    labels=['750', '850']\n",
    "    imgs = np.zeros([2048/degrade, 2048/degrade, nct, len(labels)])\n",
    "    for j, label in enumerate(labels):\n",
    "        for i in range(nct):\n",
    "            filename = whereami + '/' + path + 'smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.'+label+'_4096xy_000'+ np.str(i+1) +'r_0029p_0100z_og.gre.fit'\n",
    "            if display: print(\"i: %d  j: %d  name: %s\" % (i, j, 'smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.'+label+'_4096xy_000'+ np.str(i+1) +'r_0029p_0100z_og.gre.fit'))\n",
    "            f = fits.open(filename)\n",
    "            imgs[:,:,i,j]=rebin(f[0].data, [2048/degrade, 2048/degrade])\n",
    "            \n",
    "    return imgs, labels\n",
    "\n",
    "def slice_data(data, labels, exp_cut, exp_nshift):\n",
    "    labels=['750', '850']\n",
    "    # how many panels across\n",
    "    npanelx = 2**exp_cut\n",
    "    # and how big are they?\n",
    "    panelw = 2048/(degrade*npanelx)\n",
    "    # how many shifted panels?\n",
    "    nshift = 2**exp_nshift -1\n",
    "    # and what are the shifts?\n",
    "    shiftw =  panelw/2**exp_nshift\n",
    "    # with 4 rotations, and 2 shifts, we have\n",
    "    imgs = np.zeros([panelw, panelw, nct,(npanelx**2 +(npanelx-1)**2*nshift**2)*8, len(labels)])\n",
    "    # let's figure out where the centers are, and save that data\n",
    "    x_centers = np.zeros([nct,(npanelx**2 +(npanelx-1)**2*nshift**2)*8, len(labels)])\n",
    "    y_centers = np.zeros([nct,(npanelx**2 +(npanelx-1)**2*nshift**2)*8, len(labels)])\n",
    "    for j, label in enumerate(labels):\n",
    "        for i in range(nct):\n",
    "            q=0\n",
    "            for k in range(npanelx):\n",
    "                for l in range(npanelx):\n",
    "                    for r in range(4):\n",
    "                        imgs[:,:,i,q,j] = np.rot90(data[panelw*k:panelw*(k+1),panelw*l:panelw*(l+1),i, j], r)\n",
    "                        x_centers[i,q,j] = (panelw*k+panelw*(k+1))/2.\n",
    "                        y_centers[i,q,j] = (panelw*l+panelw*(l+1))/2.\n",
    "                        q+=1\n",
    "                        imgs[:,:,i,q,j] = np.fliplr(np.rot90(data[panelw*k:panelw*(k+1),panelw*l:panelw*(l+1),i, j], r))\n",
    "                        x_centers[i,q,j] = (panelw*k+panelw*(k+1))/2.\n",
    "                        y_centers[i,q,j] = (panelw*l+panelw*(l+1))/2.\n",
    "                        q+=1\n",
    "            for k in range(npanelx-1):\n",
    "                for l in range(npanelx-1):\n",
    "                    for m in range(nshift):\n",
    "                        for n in range(nshift):\n",
    "                            for r in range(4):\n",
    "                                imgs[:,:,i,q,j] = np.rot90(data[panelw*k+m*shiftw:panelw*(k+1)+m*shiftw,panelw*l+n*shiftw:panelw*(l+1)+n*shiftw,i, j], r)\n",
    "                                x_centers[i,q,j] = (panelw*k+m*shiftw+panelw*(k+1)+m*shiftw)/2.\n",
    "                                y_centers[i,q,j] = (panelw*l+n*shiftw+panelw*(l+1)+n*shiftw)/2.\n",
    "                                q+=1\n",
    "                                imgs[:,:,i,q,j] = np.fliplr(np.rot90(data[panelw*k+m*shiftw:panelw*(k+1)+m*shiftw,panelw*l+n*shiftw:panelw*(l+1)+n*shiftw,i, j], r))\n",
    "                                x_centers[i,q,j] = (panelw*k+m*shiftw+panelw*(k+1)+m*shiftw)/2.\n",
    "                                y_centers[i,q,j] = (panelw*l+n*shiftw+panelw*(l+1)+n*shiftw)/2.\n",
    "                                q+=1\n",
    "    return imgs, x_centers, y_centers\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the paths to the raw data files\n",
    "whereami = '/home/jhargis'\n",
    "#whereami = '/Users/jhargis'\n",
    "path     = 'Dropbox/astroNN/wl_maps/'\n",
    "#whereami = '/Users/goldston'\n",
    "#whereami = '/Users/jegpeek'\n",
    "#path = 'Documents/Weak_Lensing/kmaps_smoothed/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0001r_0029p_0100z_og.gre.fit\n",
      "i: 1  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0002r_0029p_0100z_og.gre.fit\n",
      "i: 2  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0003r_0029p_0100z_og.gre.fit\n",
      "i: 3  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0004r_0029p_0100z_og.gre.fit\n",
      "i: 4  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0005r_0029p_0100z_og.gre.fit\n",
      "i: 5  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0006r_0029p_0100z_og.gre.fit\n",
      "i: 6  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0007r_0029p_0100z_og.gre.fit\n",
      "i: 7  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0008r_0029p_0100z_og.gre.fit\n",
      "i: 8  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0009r_0029p_0100z_og.gre.fit\n",
      "i: 0  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0001r_0029p_0100z_og.gre.fit\n",
      "i: 1  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0002r_0029p_0100z_og.gre.fit\n",
      "i: 2  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0003r_0029p_0100z_og.gre.fit\n",
      "i: 3  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0004r_0029p_0100z_og.gre.fit\n",
      "i: 4  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0005r_0029p_0100z_og.gre.fit\n",
      "i: 5  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0006r_0029p_0100z_og.gre.fit\n",
      "i: 6  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0007r_0029p_0100z_og.gre.fit\n",
      "i: 7  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0008r_0029p_0100z_og.gre.fit\n",
      "i: 8  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0009r_0029p_0100z_og.gre.fit\n",
      "Data shape : (256, 256, 9, 2)\n",
      "Labels     : ['750', '850']\n"
     ]
    }
   ],
   "source": [
    "# Set (1) the factor by which we want to degrade the original WL maps\n",
    "# and (2) the number of realizations of each universe.\n",
    "#\n",
    "#   The original images are 2048 x 2048, and we degrade them using\n",
    "#   an 8 x 8 sq.pix box, which makes a smaller set of 64 images \n",
    "#   (= 256x256 sq.pix in size).\n",
    "\n",
    "degrade=8\n",
    "nct = 9\n",
    "\n",
    "data, labels = read_WL(path,display=True)\n",
    "\n",
    "print \"Data shape :\",data.shape\n",
    "print \"Labels     :\",labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display an example of the full 2048 x 2048 image\n",
    "#fullimage = getFITS(\"smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0001r_0029p_0100z_og.gre.fit\")\n",
    "#plt.imshow(fullimage, origin=\"lower\")\n",
    "#plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now compare a small section of the original image to the rebinned version\n",
    "#   Becaused we used an 8x8 box, a 256x256 region in the original image\n",
    "#   should correspond to a 32x32 region in the rebinned image\n",
    "#plt.imshow(fullimage[:256,:256],origin=\"lower\")\n",
    "#plt.colorbar()\n",
    "#plt.show()\n",
    "\n",
    "#image_data = data[:32,:32,0,0]\n",
    "#image_data.shape\n",
    "#plt.imshow(image_data, origin='lower')\n",
    "#plt.colorbar()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:9: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:10: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:11: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:13: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:16: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:17: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:18: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:20: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:23: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:24: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:25: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:27: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "# First we slice the data in the following manner:\n",
    "#\n",
    "#   1) Each 256 x 256 image is again split 8 x 8 into 64 images which are 32 x 32 sq. pix in size\n",
    "#   2) Series of 4 rotations, 2 flips, and shifts of 2 pixels in size\n",
    "imgs2, x_centers, y_centers = slice_data(data, labels, 3, 3)\n",
    "img2sh = imgs2.shape\n",
    "\n",
    "# Next, reshape the arrays and take the first 7 realizations as the training data set\n",
    "train_dataset = np.transpose(imgs2[:, :, 0:7, :, :].reshape(img2sh[0], img2sh[1], 7.0*img2sh[3]*2.0), (2, 0, 1))\n",
    "train_xc = x_centers[0:7, :, :].reshape(7.0*img2sh[3]*2.0)\n",
    "train_yc = y_centers[0:7, :, :].reshape(7.0*img2sh[3]*2.0)\n",
    "ones = np.ones([7,img2sh[3], 2] )\n",
    "train_labels = ((np.asarray([0,1])).reshape(1, 1, 2)*ones).reshape(7.0*img2sh[3]*2.0)\n",
    "\n",
    "# The validation set is the 8th realization\n",
    "valid_dataset = np.transpose(imgs2[:, :, 7, :, :].reshape(img2sh[0], img2sh[1], 1.0*img2sh[3]*2.0), (2, 0, 1))\n",
    "valid_xc = x_centers[7, :, :].reshape(1.0*img2sh[3]*2.0)\n",
    "valid_yc = y_centers[7, :, :].reshape(1.0*img2sh[3]*2.0)\n",
    "ones = np.ones([1,img2sh[3], 2] )\n",
    "valid_labels = ((np.asarray([0,1])).reshape(1, 1, 2)*ones).reshape(1.0*img2sh[3]*2.0)\n",
    "\n",
    "# The test data set is the 9th realization\n",
    "test_dataset = np.transpose(imgs2[:, :, 8, :, :].reshape(img2sh[0], img2sh[1], 1.0*img2sh[3]*2.0), (2, 0, 1))\n",
    "test_xc = x_centers[8, :, :].reshape(1.0*img2sh[3]*2.0)\n",
    "test_yc = y_centers[8, :, :].reshape(1.0*img2sh[3]*2.0)\n",
    "ones = np.ones([1,img2sh[3], 2] )\n",
    "test_labels = ((np.asarray([0,1])).reshape(1, 1, 2)*ones).reshape(1.0*img2sh[3]*2.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master images tensor shape: (32, 32, 9, 19720, 2)\n",
      "\n",
      "Train dataset shape: (276080, 32, 32)\n",
      "Train labels shape : (276080,)\n",
      "Test dataset shape : (39440, 32, 32)\n",
      "Valid dataset shape: (39440, 32, 32)\n",
      "TOTAL data sets    :  354960\n"
     ]
    }
   ],
   "source": [
    "print \"Master images tensor shape:\", img2sh\n",
    "print\n",
    "print \"Train dataset shape:\", train_dataset.shape\n",
    "print \"Train labels shape :\", train_labels.shape\n",
    "print \"Test dataset shape :\", test_dataset.shape\n",
    "print \"Valid dataset shape:\", valid_dataset.shape\n",
    "print \"TOTAL data sets    : \", train_dataset.shape[0] + test_dataset.shape[0] + valid_dataset.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbbd7cd0c90>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6EvSjFj0JWlGLPqSNCP/D76+BEMbBYhxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbc87e9ec10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.reshape(x_centers, 9*19720*2), np.reshape(y_centers, 9*19720*2), '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#pickle_file = 'notMNIST.pickle'\\n#pickle_file = '/Users/jegpeek/Documents/WL88.pickle'\\npickle_file = '/Users/jegpeek/Dropbox/WL_other.pickle'\\n\\nusePickle = True\\n\\nif usePickle:\\n    with open(pickle_file, 'rb') as f:\\n      save = pickle.load(f)\\n      train_dataset = save['train_dataset']\\n      train_labels = save['train_labels']\\n      valid_dataset = save['valid_dataset']\\n      valid_labels = save['valid_labels']\\n      test_dataset = save['test_dataset']\\n      test_labels = save['test_labels']\\n      del save  # hint to help gc free up memory\\n      print('Training set', train_dataset.shape, train_labels.shape)\\n      print('Validation set', valid_dataset.shape, valid_labels.shape)\\n      print('Test set', test_dataset.shape, test_labels.shape)\\nelse:\\n    %run Read_WL.py\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just don't even worry about this for now.\n",
    "# Figure it out later.\n",
    "\n",
    "\"\"\"\n",
    "#pickle_file = 'notMNIST.pickle'\n",
    "#pickle_file = '/Users/jegpeek/Documents/WL88.pickle'\n",
    "pickle_file = '/Users/jegpeek/Dropbox/WL_other.pickle'\n",
    "\n",
    "usePickle = True\n",
    "\n",
    "if usePickle:\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "      save = pickle.load(f)\n",
    "      train_dataset = save['train_dataset']\n",
    "      train_labels = save['train_labels']\n",
    "      valid_dataset = save['valid_dataset']\n",
    "      valid_labels = save['valid_labels']\n",
    "      test_dataset = save['test_dataset']\n",
    "      test_labels = save['test_labels']\n",
    "      del save  # hint to help gc free up memory\n",
    "      print('Training set', train_dataset.shape, train_labels.shape)\n",
    "      print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "      print('Test set', test_dataset.shape, test_labels.shape)\n",
    "else:\n",
    "    %run Read_WL.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (276080, 32, 32, 1), (276080, 2))\n",
      "('Validation set', (39440, 32, 32, 1), (39440, 2))\n",
      "('Test set', (39440, 32, 32, 1), (39440, 2))\n"
     ]
    }
   ],
   "source": [
    "# Reformat into a TensorFlow-friendly shape:\n",
    "# - convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "# - labels as float 1-hot encodings.\n",
    "\n",
    "image_size = 32\n",
    "num_labels = 2\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cut this down to a subset\n",
    "#test_frac = 0.3\n",
    "#_train_dataset = train_dataset[0:train_dataset.shape[0]*test_frac,:,:,:]\n",
    "#_train_labels  = train_labels[0:train_labels.shape[0]*test_frac,:]\n",
    "#_test_dataset = test_dataset[0:test_dataset.shape[0]*test_frac,:,:,:]\n",
    "##_test_labels  = test_labels[0:test_labels.shape[0]*test_frac,:]\n",
    "#_valid_dataset = valid_dataset[0:valid_dataset.shape[0]*test_frac,:,:,:]\n",
    "#_valid_labels  = valid_labels[0:valid_labels.shape[0]*test_frac,:]\n",
    "#print('--> Using subset <--')\n",
    "#print('Training set', _train_dataset.shape, _train_labels.shape)\n",
    "#print('Validation set', _valid_dataset.shape, _valid_labels.shape)\n",
    "#print('Test set', _test_dataset.shape, _test_labels.shape)\n",
    "#train_dataset = _train_dataset\n",
    "#train_labels = _train_labels\n",
    "#test_dataset = _test_dataset\n",
    "#test_labels = _test_labels\n",
    "#valid_dataset = _valid_dataset\n",
    "#valid_labels = _valid_labels\n",
    "#print('Training set', train_dataset.shape, train_labels.shape)\n",
    "#print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "#print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "IZYv70SvvOan"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (128, 32, 32, 1)\n",
      "conv1 shape: (128, 16, 16, 32)\n",
      "hidden1 shape: (128, 16, 16, 32)\n",
      "Training\n",
      "--------\n",
      "Validation\n",
      "----------\n",
      "data shape: (39440, 32, 32, 1)\n",
      "conv1 shape: (39440, 16, 16, 32)\n",
      "hidden1 shape: (39440, 16, 16, 32)\n",
      "Test\n",
      "----\n",
      "data shape: (39440, 32, 32, 1)\n",
      "conv1 shape: (39440, 16, 16, 32)\n",
      "hidden1 shape: (39440, 16, 16, 32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128  # 16\n",
    "patch_size = 5    # 5\n",
    "depth = 32        # 16\n",
    "depth2= 64        # JRH addition\n",
    "depth3=128        # JRH addition\n",
    "num_hidden = 2048   # 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Global Step\n",
    "    #global_step = tf.Variable(1.)\n",
    "    #learn_decay = 0.85\n",
    "    #learning_rate = tf.train.exponential_decay(0.005, global_step, 10000, learn_decay, staircase=True)\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    # Variables.\n",
    "    #layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    #layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    #layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "    #layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    #layer3_weights = tf.Variable(tf.truncated_normal([image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "    #layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    #layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "    #layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "    # Alternate variable setup\n",
    "    #   Layer 1: Compute 16 features = depth\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    #layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    layer1_biases = tf.Variable(tf.constant(0.1, shape=[depth]))\n",
    "    hidden1 = tf.Variable(tf.zeros([batch_size, image_size // 2, image_size // 2, depth]))\n",
    "    \n",
    "    #   Layer 2: Compute 32 features = DEPTH2\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth2], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(0.1, shape=[depth2]))\n",
    "    \n",
    "    #   Layer 2b: Compute 128 features = DEPTH3\n",
    "    layer2b_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth2, depth3], stddev=0.1))\n",
    "    layer2b_biases = tf.Variable(tf.constant(0.1, shape=[depth3]))\n",
    "    \n",
    "    #   Layer 3: Fully-connected layer should use depth2, which results from layer2\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal([image_size // 8 * image_size // 8 * depth3, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(0.01, shape=[num_hidden]))\n",
    "    \n",
    "    #   Layer 4: Readout layer\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(0.01, shape=[num_labels]))\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    #with tf.name_scope('derp'):\n",
    "    #  spl = tf.split(3, 16, layer1_weights)\n",
    "    #  filter_summary = tf.image_summary((spl[0]).name, spl[0], max_images=1)\n",
    "\n",
    "    # Model.\n",
    "    def model(data):\n",
    "        \n",
    "        print \"data shape:\", data.get_shape()\n",
    "        #  Layer 1\n",
    "        conv1 = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "        print \"conv1 shape:\", conv1.get_shape()\n",
    "        print \"hidden1 shape:\", hidden1.get_shape()\n",
    "        #  Layer 2\n",
    "        conv2 = tf.nn.conv2d(hidden1, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden2 = tf.nn.relu(conv2 + layer2_biases)\n",
    "        #  Layer 2b\n",
    "        conv2b = tf.nn.conv2d(hidden2, layer2b_weights, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden2b = tf.nn.relu(conv2b + layer2b_biases)\n",
    "        #  Layer 3 = fully connected\n",
    "        shape = hidden2b.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden2b, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden3 = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        #  Layer 4 = readout layer\n",
    "        return tf.matmul(hidden3, layer4_weights) + layer4_biases\n",
    "\n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "    def variable_summaries(var, name):\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.scalar_summary('mean/' + name, mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n",
    "        tf.scalar_summary('sttdev/' + name, stddev)\n",
    "        tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "        tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "        tf.histogram_summary(name, var)\n",
    "        \n",
    "    # Visualize the input data.  Shape = [128, 32, 32, 1]\n",
    "    with tf.name_scope('input_data'):\n",
    "        with tf.name_scope('images'):\n",
    "            tf.image_summary(\"input\", tf_train_dataset)\n",
    "        \n",
    "    with tf.name_scope('layer1'):\n",
    "        # This Variable will hold the state of the weights for the layer\n",
    "        with tf.name_scope(\"weights\"):\n",
    "            variable_summaries(layer1_weights, 'layer1/weights')\n",
    "            with tf.name_scope(\"images\"):\n",
    "                #spl = tf.split(3, 8, layer1_weights)\n",
    "                #tf.image_summary(\"layer1/weights\", spl[0])\n",
    "                layer1_weights_reshape = tf.reshape(layer1_weights, [depth,patch_size,patch_size,1])\n",
    "                tf.image_summary(\"layer1/weights\", layer1_weights_reshape)\n",
    "        with tf.name_scope(\"biases\"):\n",
    "            variable_summaries(layer1_biases, 'layer1/biases')\n",
    "        with tf.name_scope(\"hidden\"):\n",
    "            #spl = tf.split(0, batch_size, hidden1)\n",
    "            #layer1_hidden_reshape = tf.reshape(hidden1, [-1,image_size // 2,image_size // 2,1])\n",
    "            #print\"hidden_reshape:\",layer1_hidden_reshape.get_shape()\n",
    "            #tf.image_summary(\"layer1/hidden\", layer1_hidden_reshape)\n",
    "            spl = tf.split(0, batch_size, hidden1)\n",
    "            layer1_hidden_reshape = tf.reshape(spl[0], [depth, image_size // 2, image_size // 2, 1 ])\n",
    "            tf.image_summary(\"layer1/hidden\",layer1_hidden_reshape,max_images=32)\n",
    "            #print\"hidden_reshape:\",layer1_hidden_reshape.get_shape()\n",
    "            #print \"spl shape:\",spl[0].get_shape()\n",
    "            \n",
    "    with tf.name_scope('layer2'):\n",
    "        # This Variable will hold the state of the weights for the layer\n",
    "        with tf.name_scope(\"weights\"):\n",
    "            variable_summaries(layer2_weights, 'layer2/weights')\n",
    "            with tf.name_scope(\"images\"):\n",
    "                spl = tf.split(3, 16, layer2_weights)\n",
    "                tf.image_summary(\"layer2/weights\", spl[0])\n",
    "        with tf.name_scope(\"biases\"):\n",
    "            variable_summaries(layer2_biases, 'layer2/biases')\n",
    "            \n",
    "    with tf.name_scope('layer2b'):\n",
    "        # This Variable will hold the state of the weights for the layer\n",
    "        with tf.name_scope(\"weights\"):\n",
    "            variable_summaries(layer2b_weights, 'layer2b/weights')\n",
    "            with tf.name_scope(\"images\"):\n",
    "                spl = tf.split(3, 32, layer2b_weights)\n",
    "                tf.image_summary(\"layer2b/weights\", spl[0])\n",
    "        with tf.name_scope(\"biases\"):\n",
    "            variable_summaries(layer2b_biases, 'layer2b/biases')\n",
    "\n",
    "    with tf.name_scope('layer3'):\n",
    "        # This Variable will hold the state of the weights for the layer\n",
    "        with tf.name_scope(\"weights\"):\n",
    "            variable_summaries(layer2_weights, 'layer3/weights')\n",
    "        with tf.name_scope(\"biases\"):\n",
    "            variable_summaries(layer3_biases, 'layer3/biases')\n",
    "            \n",
    "    with tf.name_scope('layer4'):\n",
    "        # This Variable will hold the state of the weights for the layer\n",
    "        with tf.name_scope(\"weights\"):\n",
    "            variable_summaries(layer4_weights, 'layer4/weights')\n",
    "        with tf.name_scope(\"biases\"):\n",
    "            variable_summaries(layer4_biases, 'layer4/biases')\n",
    "            \n",
    "                \n",
    "    with tf.name_scope('loss'):\n",
    "        # Try to log the loss\n",
    "        tf.scalar_summary('loss', loss)\n",
    "        \n",
    "    with tf.name_scope('learning_rate'):\n",
    "        tf.scalar_summary('learning_rate', learning_rate)\n",
    "                \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    print \"Training\"\n",
    "    print \"--------\"\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    print \"Validation\"\n",
    "    print \"----------\"\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    print \"Test\"\n",
    "    print \"----\"\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))\n",
    "\n",
    "    merged = tf.merge_all_summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "permutation = np.random.permutation(train_labels.shape[0])\n",
    "train_dataset = train_dataset[permutation,:,:]\n",
    "train_labels = train_labels[permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAD7CAYAAACc26SuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCOZQHbUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbbbc469b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 1200: 0.675547 (0.046 sec)\n"
     ]
    }
   ],
   "source": [
    "# Run of TensorFlow without the permutations\n",
    "# but with the TensorBoard logging calls\n",
    "num_steps = 200001\n",
    "print_step = 200\n",
    "summary_step = 200\n",
    "losses = np.zeros((num_steps-1)/print_step+1)\n",
    "acc_valid = np.zeros((num_steps-1)/print_step+1)\n",
    "acc_test = np.zeros((num_steps-1)/print_step+1)\n",
    "acc_train = np.zeros((num_steps-1)/print_step+1)\n",
    "q = 0\n",
    "p = 0\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    summary_writer = tf.train.SummaryWriter(whereami+'/Documents/logs', session.graph_def)\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        summary, _, l, predictions = session.run([merged, optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        duration = time.time() - start_time\n",
    "        if (step % print_step == 0):\n",
    "            losses[q] = l\n",
    "            acc_valid[q] = accuracy(valid_prediction.eval(), valid_labels)/100.0\n",
    "            acc_test[q] = accuracy(test_prediction.eval(), test_labels)/100.0\n",
    "            acc_train[q] = accuracy(predictions, batch_labels)/100.0\n",
    "            q += 1\n",
    "            plt.plot(np.arange(0,(num_steps-1)/print_step+1), acc_valid, '.', color='b',alpha=0.5)\n",
    "            plt.plot((-1)*np.arange(0,(num_steps-1)/print_step+1),acc_test, '.', color='g',alpha=0.5)\n",
    "            plt.plot(np.arange(0,(num_steps-1)/print_step+1), acc_train, '.', color='r',alpha=0.5)\n",
    "            plt.ylim([0.45, 1.05])\n",
    "            #plt.xlim([-1000, 1000])\n",
    "            plt.xlim([-1*((step/print_step)*1.1),(step/print_step)*1.1])\n",
    "            #plt.xlim([-1*(((num_steps-1)/print_step+1)*1.1),((num_steps-1)/print_step+1)*1.1])\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "            print('Minibatch loss at step %d: %f (%.3f sec)' % (step, l, duration))\n",
    "            #print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            #print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "        if (step % summary_step == 0):\n",
    "            #print \"got here\"\n",
    "            summary_writer.add_summary(summary, p)\n",
    "            p += 1\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# permutation = np.random.permutation(train_labels.shape[0])\n",
    "#train_dataset = train_dataset[permutation,:,:]\n",
    "#train_labels = train_labels[permutation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "num_steps = 20001\n",
    "print_step = 200\n",
    "losses = np.zeros((num_steps-1)/print_step+1)\n",
    "acc_valid = np.zeros((num_steps-1)/print_step+1)\n",
    "acc_test = np.zeros((num_steps-1)/print_step+1)\n",
    "q = 0\n",
    "p=0\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % print_step == 0):\n",
    "            losses[q] = l\n",
    "            acc_valid[q] = accuracy(valid_prediction.eval(), valid_labels)/100.0\n",
    "            acc_test[q] = accuracy(test_prediction.eval(), test_labels)/100.0\n",
    "            q += 1\n",
    "            plt.plot(np.arange(0,(num_steps-1)/print_step+1), acc_valid, '.', color='b')\n",
    "            plt.plot((-1)*np.arange(0,(num_steps-1)/print_step+1),acc_test, '.', color='g')\n",
    "            plt.ylim([0.45, 0.65])\n",
    "            plt.xlim([-1000, 1000])\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            #print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            #print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "plt.plot(np.arange(0,(num_steps-1)/print_step+1), acc_valid, '.', color='b')\n",
    "plt.plot((-1)*np.arange(0,(num_steps-1)/print_step+1),acc_test, '.', color='g')\n",
    "plt.ylim([0.45, 0.75])\n",
    "plt.xlim([-100, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(acc_test, acc_valid, '.', color='b')\n",
    "#plt.plot((-1)*np.arange(0,(num_steps-1)/print_step+1),losses, '.', color='g')\n",
    "plt.ylim([0.4, 0.9])\n",
    "plt.xlim([0.4, 0.9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Open questions:**\n",
    "+ Why is there so much scatter in the loss function over time?\n",
    "+ Is there structure in the loss function over time?\n",
    "+ If I plot loss vs. accuracy, what do I get? \n",
    "+ Do I really see a difference when I scramble vs. leave in order, and if so, is it because of the way SGD interacts with the two cosmologies?\n",
    "+ will deeper / better networks get us over 65%?\n",
    "+ Why, oh why, are my test and valid data sets so damn well correlated??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "graph.get_tensor_by_name.im_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KedKkn4EutIK"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klf21gpbAgb-"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
