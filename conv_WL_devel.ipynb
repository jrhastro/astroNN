{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Investigating using Convolutional Networks on Weak Lensing data\n",
    "=============\n",
    "\n",
    "Adapted from 4_conv_WL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "plt.rcParams['image.cmap'] = 'viridis'\n",
    "plt.rcParams['image.interpolation'] = 'none'\n",
    "%matplotlib inline\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reorganize the code a bit by putting the function definitions first.\n",
    "def rebin(a, shape):\n",
    "    sh = shape[0],a.shape[0]//shape[0],shape[1],a.shape[1]//shape[1]\n",
    "    return a.reshape(sh).mean(-1).mean(1)\n",
    "\n",
    "def getFITS(imagename):\n",
    "    filename = whereami + '/' + path + imagename\n",
    "    f = fits.open(filename)\n",
    "    dataout = f[0].data\n",
    "    \n",
    "    return dataout\n",
    "\n",
    "def read_WL(path,display=None):\n",
    "    # this is a version to look at sigma8\n",
    "    labels=['750', '850']\n",
    "    imgs = np.zeros([2048/degrade, 2048/degrade, nct, len(labels)])\n",
    "    for j, label in enumerate(labels):\n",
    "        for i in range(nct):\n",
    "            filename = whereami + '/' + path + 'smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.'+label+'_4096xy_000'+ np.str(i+1) +'r_0029p_0100z_og.gre.fit'\n",
    "            if display: print(\"i: %d  j: %d  name: %s\" % (i, j, 'smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.'+label+'_4096xy_000'+ np.str(i+1) +'r_0029p_0100z_og.gre.fit'))\n",
    "            f = fits.open(filename)\n",
    "            imgs[:,:,i,j]=rebin(f[0].data, [2048/degrade, 2048/degrade])\n",
    "            \n",
    "    return imgs, labels\n",
    "\n",
    "def slice_data(data, labels, exp_cut, exp_nshift):\n",
    "    labels=['750', '850']\n",
    "    # how many panels across\n",
    "    npanelx = 2**exp_cut\n",
    "    # and how big are they?\n",
    "    panelw = 2048/(degrade*npanelx)\n",
    "    # how many shifted panels?\n",
    "    nshift = 2**exp_nshift -1\n",
    "    # and what are the shifts?\n",
    "    shiftw =  panelw/2**exp_nshift\n",
    "    # with 4 rotations, and 2 shifts, we have\n",
    "    imgs = np.zeros([panelw, panelw, nct,(npanelx**2 +(npanelx-1)**2*nshift**2)*8, len(labels)])\n",
    "    # let's figure out where the centers are, and save that data\n",
    "    x_centers = np.zeros([nct,(npanelx**2 +(npanelx-1)**2*nshift**2)*8, len(labels)])\n",
    "    y_centers = np.zeros([nct,(npanelx**2 +(npanelx-1)**2*nshift**2)*8, len(labels)])\n",
    "    for j, label in enumerate(labels):\n",
    "        for i in range(nct):\n",
    "            q=0\n",
    "            for k in range(npanelx):\n",
    "                for l in range(npanelx):\n",
    "                    for r in range(4):\n",
    "                        imgs[:,:,i,q,j] = np.rot90(data[panelw*k:panelw*(k+1),panelw*l:panelw*(l+1),i, j], r)\n",
    "                        x_centers[i,q,j] = (panelw*k+panelw*(k+1))/2.\n",
    "                        y_centers[i,q,j] = (panelw*l+panelw*(l+1))/2.\n",
    "                        q+=1\n",
    "                        imgs[:,:,i,q,j] = np.fliplr(np.rot90(data[panelw*k:panelw*(k+1),panelw*l:panelw*(l+1),i, j], r))\n",
    "                        x_centers[i,q,j] = (panelw*k+panelw*(k+1))/2.\n",
    "                        y_centers[i,q,j] = (panelw*l+panelw*(l+1))/2.\n",
    "                        q+=1\n",
    "            for k in range(npanelx-1):\n",
    "                for l in range(npanelx-1):\n",
    "                    for m in range(nshift):\n",
    "                        for n in range(nshift):\n",
    "                            for r in range(4):\n",
    "                                imgs[:,:,i,q,j] = np.rot90(data[panelw*k+m*shiftw:panelw*(k+1)+m*shiftw,panelw*l+n*shiftw:panelw*(l+1)+n*shiftw,i, j], r)\n",
    "                                x_centers[i,q,j] = (panelw*k+m*shiftw+panelw*(k+1)+m*shiftw)/2.\n",
    "                                y_centers[i,q,j] = (panelw*l+n*shiftw+panelw*(l+1)+n*shiftw)/2.\n",
    "                                q+=1\n",
    "                                imgs[:,:,i,q,j] = np.fliplr(np.rot90(data[panelw*k+m*shiftw:panelw*(k+1)+m*shiftw,panelw*l+n*shiftw:panelw*(l+1)+n*shiftw,i, j], r))\n",
    "                                x_centers[i,q,j] = (panelw*k+m*shiftw+panelw*(k+1)+m*shiftw)/2.\n",
    "                                y_centers[i,q,j] = (panelw*l+n*shiftw+panelw*(l+1)+n*shiftw)/2.\n",
    "                                q+=1\n",
    "    return imgs, x_centers, y_centers\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the paths to the raw data files\n",
    "whereami = '/home/jhargis'\n",
    "#whereami = '/Users/jhargis'\n",
    "path     = 'Dropbox/astroNN/wl_maps/'\n",
    "#whereami = '/Users/goldston'\n",
    "#whereami = '/Users/jegpeek'\n",
    "#path = 'Documents/Weak_Lensing/kmaps_smoothed/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0001r_0029p_0100z_og.gre.fit\n",
      "i: 1  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0002r_0029p_0100z_og.gre.fit\n",
      "i: 2  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0003r_0029p_0100z_og.gre.fit\n",
      "i: 3  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0004r_0029p_0100z_og.gre.fit\n",
      "i: 4  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0005r_0029p_0100z_og.gre.fit\n",
      "i: 5  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0006r_0029p_0100z_og.gre.fit\n",
      "i: 6  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0007r_0029p_0100z_og.gre.fit\n",
      "i: 7  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0008r_0029p_0100z_og.gre.fit\n",
      "i: 8  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0009r_0029p_0100z_og.gre.fit\n",
      "i: 0  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0001r_0029p_0100z_og.gre.fit\n",
      "i: 1  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0002r_0029p_0100z_og.gre.fit\n",
      "i: 2  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0003r_0029p_0100z_og.gre.fit\n",
      "i: 3  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0004r_0029p_0100z_og.gre.fit\n",
      "i: 4  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0005r_0029p_0100z_og.gre.fit\n",
      "i: 5  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0006r_0029p_0100z_og.gre.fit\n",
      "i: 6  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0007r_0029p_0100z_og.gre.fit\n",
      "i: 7  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0008r_0029p_0100z_og.gre.fit\n",
      "i: 8  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0009r_0029p_0100z_og.gre.fit\n",
      "Data shape : (256, 256, 9, 2)\n",
      "Labels     : ['750', '850']\n"
     ]
    }
   ],
   "source": [
    "# Set (1) the factor by which we want to degrade the original WL maps\n",
    "# and (2) the number of realizations of each universe.\n",
    "#\n",
    "#   The original images are 2048 x 2048, and we degrade them using\n",
    "#   an 8 x 8 sq.pix box, which makes a smaller set of 64 images \n",
    "#   (= 256x256 sq.pix in size).\n",
    "\n",
    "degrade=8\n",
    "nct = 9\n",
    "\n",
    "data, labels = read_WL(path,display=True)\n",
    "\n",
    "print \"Data shape :\",data.shape\n",
    "print \"Labels     :\",labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display an example of the full 2048 x 2048 image\n",
    "#fullimage = getFITS(\"smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0001r_0029p_0100z_og.gre.fit\")\n",
    "#plt.imshow(fullimage, origin=\"lower\")\n",
    "#plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now compare a small section of the original image to the rebinned version\n",
    "#   Becaused we used an 8x8 box, a 256x256 region in the original image\n",
    "#   should correspond to a 32x32 region in the rebinned image\n",
    "#plt.imshow(fullimage[:256,:256],origin=\"lower\")\n",
    "#plt.colorbar()\n",
    "#plt.show()\n",
    "\n",
    "#image_data = data[:32,:32,0,0]\n",
    "#image_data.shape\n",
    "#plt.imshow(image_data, origin='lower')\n",
    "#plt.colorbar()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:9: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:10: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:11: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:13: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:16: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:17: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:18: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:20: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:23: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:24: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:25: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:27: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "# First we slice the data in the following manner:\n",
    "#\n",
    "#   1) Each 256 x 256 image is again split 8 x 8 into 64 images which are 32 x 32 sq. pix in size\n",
    "#   2) Series of 4 rotations, 2 flips, and shifts of 2 pixels in size\n",
    "imgs2, x_centers, y_centers = slice_data(data, labels, 3, 3)\n",
    "img2sh = imgs2.shape\n",
    "\n",
    "# Next, reshape the arrays and take the first 7 realizations as the training data set\n",
    "train_dataset = np.transpose(imgs2[:, :, 0:7, :, :].reshape(img2sh[0], img2sh[1], 7.0*img2sh[3]*2.0), (2, 0, 1))\n",
    "train_xc = x_centers[0:7, :, :].reshape(7.0*img2sh[3]*2.0)\n",
    "train_yc = y_centers[0:7, :, :].reshape(7.0*img2sh[3]*2.0)\n",
    "ones = np.ones([7,img2sh[3], 2] )\n",
    "train_labels = ((np.asarray([0,1])).reshape(1, 1, 2)*ones).reshape(7.0*img2sh[3]*2.0)\n",
    "\n",
    "# The validation set is the 8th realization\n",
    "valid_dataset = np.transpose(imgs2[:, :, 7, :, :].reshape(img2sh[0], img2sh[1], 1.0*img2sh[3]*2.0), (2, 0, 1))\n",
    "valid_xc = x_centers[7, :, :].reshape(1.0*img2sh[3]*2.0)\n",
    "valid_yc = y_centers[7, :, :].reshape(1.0*img2sh[3]*2.0)\n",
    "ones = np.ones([1,img2sh[3], 2] )\n",
    "valid_labels = ((np.asarray([0,1])).reshape(1, 1, 2)*ones).reshape(1.0*img2sh[3]*2.0)\n",
    "\n",
    "# The test data set is the 9th realization\n",
    "test_dataset = np.transpose(imgs2[:, :, 8, :, :].reshape(img2sh[0], img2sh[1], 1.0*img2sh[3]*2.0), (2, 0, 1))\n",
    "test_xc = x_centers[8, :, :].reshape(1.0*img2sh[3]*2.0)\n",
    "test_yc = y_centers[8, :, :].reshape(1.0*img2sh[3]*2.0)\n",
    "ones = np.ones([1,img2sh[3], 2] )\n",
    "test_labels = ((np.asarray([0,1])).reshape(1, 1, 2)*ones).reshape(1.0*img2sh[3]*2.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master images tensor shape: (32, 32, 9, 19720, 2)\n",
      "\n",
      "Train dataset shape: (276080, 32, 32)\n",
      "Train labels shape : (276080,)\n",
      "Test dataset shape : (39440, 32, 32)\n",
      "Valid dataset shape: (39440, 32, 32)\n",
      "TOTAL data sets    :  354960\n"
     ]
    }
   ],
   "source": [
    "print \"Master images tensor shape:\", img2sh\n",
    "print\n",
    "print \"Train dataset shape:\", train_dataset.shape\n",
    "print \"Train labels shape :\", train_labels.shape\n",
    "print \"Test dataset shape :\", test_dataset.shape\n",
    "print \"Valid dataset shape:\", valid_dataset.shape\n",
    "print \"TOTAL data sets    : \", train_dataset.shape[0] + test_dataset.shape[0] + valid_dataset.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa20ca9bb90>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6EvSjFj0JWlGLPqSNCP/D76+BEMbBYhxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa20cae2dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.reshape(x_centers, 9*19720*2), np.reshape(y_centers, 9*19720*2), '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#pickle_file = 'notMNIST.pickle'\\n#pickle_file = '/Users/jegpeek/Documents/WL88.pickle'\\npickle_file = '/Users/jegpeek/Dropbox/WL_other.pickle'\\n\\nusePickle = True\\n\\nif usePickle:\\n    with open(pickle_file, 'rb') as f:\\n      save = pickle.load(f)\\n      train_dataset = save['train_dataset']\\n      train_labels = save['train_labels']\\n      valid_dataset = save['valid_dataset']\\n      valid_labels = save['valid_labels']\\n      test_dataset = save['test_dataset']\\n      test_labels = save['test_labels']\\n      del save  # hint to help gc free up memory\\n      print('Training set', train_dataset.shape, train_labels.shape)\\n      print('Validation set', valid_dataset.shape, valid_labels.shape)\\n      print('Test set', test_dataset.shape, test_labels.shape)\\nelse:\\n    %run Read_WL.py\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just don't even worry about this for now.\n",
    "# Figure it out later.\n",
    "\n",
    "\"\"\"\n",
    "#pickle_file = 'notMNIST.pickle'\n",
    "#pickle_file = '/Users/jegpeek/Documents/WL88.pickle'\n",
    "pickle_file = '/Users/jegpeek/Dropbox/WL_other.pickle'\n",
    "\n",
    "usePickle = True\n",
    "\n",
    "if usePickle:\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "      save = pickle.load(f)\n",
    "      train_dataset = save['train_dataset']\n",
    "      train_labels = save['train_labels']\n",
    "      valid_dataset = save['valid_dataset']\n",
    "      valid_labels = save['valid_labels']\n",
    "      test_dataset = save['test_dataset']\n",
    "      test_labels = save['test_labels']\n",
    "      del save  # hint to help gc free up memory\n",
    "      print('Training set', train_dataset.shape, train_labels.shape)\n",
    "      print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "      print('Test set', test_dataset.shape, test_labels.shape)\n",
    "else:\n",
    "    %run Read_WL.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (276080, 32, 32, 1), (276080, 2))\n",
      "('Validation set', (39440, 32, 32, 1), (39440, 2))\n",
      "('Test set', (39440, 32, 32, 1), (39440, 2))\n"
     ]
    }
   ],
   "source": [
    "# Reformat into a TensorFlow-friendly shape:\n",
    "# - convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "# - labels as float 1-hot encodings.\n",
    "\n",
    "image_size = 32\n",
    "num_labels = 2\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "IZYv70SvvOan"
   },
   "outputs": [],
   "source": [
    "batch_size = 16  # 16\n",
    "patch_size = 5    # 5\n",
    "depth = 16        # 16\n",
    "num_hidden = 256   # 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Global Step\n",
    "    #global_step = tf.Variable(1.)\n",
    "    #learn_decay = 0.85\n",
    "    #learning_rate = tf.train.exponential_decay(0.005, global_step, 10000, learn_decay, staircase=True)\n",
    "    learning_rate = 0.0005\n",
    "\n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal([image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "    #with tf.name_scope('derp'):\n",
    "    #  spl = tf.split(3, 16, layer1_weights)\n",
    "    #  filter_summary = tf.image_summary((spl[0]).name, spl[0], max_images=1)\n",
    "\n",
    "    # Model.\n",
    "    def model(data):\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "\n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "    def variable_summaries(var, name):\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.scalar_summary('mean/' + name, mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n",
    "        tf.scalar_summary('sttdev/' + name, stddev)\n",
    "        tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "        tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "        tf.histogram_summary(name, var)\n",
    "        \n",
    "    with tf.name_scope('layer1'):\n",
    "        # This Variable will hold the state of the weights for the layer\n",
    "        with tf.name_scope(\"weights\"):\n",
    "            variable_summaries(layer1_weights, 'layer1/weights')\n",
    "            with tf.name_scope(\"images\"):\n",
    "                spl = tf.split(3, 16, layer1_weights)\n",
    "                tf.image_summary(\"layer1/weights\", spl[0])\n",
    "                \n",
    "    with tf.name_scope('layer2'):\n",
    "        # This Variable will hold the state of the weights for the layer\n",
    "        with tf.name_scope(\"weights\"):\n",
    "            variable_summaries(layer2_weights, 'layer2/weights')\n",
    "            with tf.name_scope(\"images\"):\n",
    "                spl = tf.split(3, 16, layer2_weights)\n",
    "                tf.image_summary(\"layer2/weights\", spl[0])\n",
    "\n",
    "                \n",
    "    with tf.name_scope('loss'):\n",
    "        # Try to log the loss\n",
    "        tf.scalar_summary('loss', loss)\n",
    "        \n",
    "    with tf.name_scope('learning_rate'):\n",
    "        tf.scalar_summary('learning_rate', learning_rate)\n",
    "                \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))\n",
    "\n",
    "    merged = tf.merge_all_summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQZIEGAiSpIaBIEkC4P8D7AIr8UkxsD4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa23c016e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run of TensorFlow without the permutations\n",
    "# but with the TensorBoard logging calls\n",
    "num_steps = 400001\n",
    "print_step = 200\n",
    "summary_step = 200\n",
    "losses = np.zeros((num_steps-1)/print_step+1)\n",
    "acc_valid = np.zeros((num_steps-1)/print_step+1)\n",
    "acc_test = np.zeros((num_steps-1)/print_step+1)\n",
    "q = 0\n",
    "p = 0\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    summary_writer = tf.train.SummaryWriter(whereami+'/Documents/logs', session.graph_def)\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        summary, _, l, predictions = session.run([merged, optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % print_step == 0):\n",
    "            losses[q] = l\n",
    "            acc_valid[q] = accuracy(valid_prediction.eval(), valid_labels)/100.0\n",
    "            acc_test[q] = accuracy(test_prediction.eval(), test_labels)/100.0\n",
    "            with tf.name_scope('test_accuracy'):\n",
    "                tf.scalar_summary('test_accuracy', acc_test[q])\n",
    "            q += 1\n",
    "            plt.plot(np.arange(0,(num_steps-1)/print_step+1), acc_valid, '.', color='b')\n",
    "            plt.plot((-1)*np.arange(0,(num_steps-1)/print_step+1),acc_test, '.', color='g')\n",
    "            plt.ylim([0.45, 0.75])\n",
    "            plt.xlim([-1000, 1000])\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            #print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            #print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "        if (step % summary_step == 0):\n",
    "            summary_writer.add_summary(summary, p)\n",
    "            p += 1\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "permutation = np.random.permutation(train_labels.shape[0])\n",
    "train_dataset = train_dataset[permutation,:,:]\n",
    "train_labels = train_labels[permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 20001\n",
    "print_step = 200\n",
    "losses = np.zeros((num_steps-1)/print_step+1)\n",
    "acc_valid = np.zeros((num_steps-1)/print_step+1)\n",
    "acc_test = np.zeros((num_steps-1)/print_step+1)\n",
    "q = 0\n",
    "p=0\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % print_step == 0):\n",
    "            losses[q] = l\n",
    "            acc_valid[q] = accuracy(valid_prediction.eval(), valid_labels)/100.0\n",
    "            acc_test[q] = accuracy(test_prediction.eval(), test_labels)/100.0\n",
    "            q += 1\n",
    "            plt.plot(np.arange(0,(num_steps-1)/print_step+1), acc_valid, '.', color='b')\n",
    "            plt.plot((-1)*np.arange(0,(num_steps-1)/print_step+1),acc_test, '.', color='g')\n",
    "            plt.ylim([0.45, 0.65])\n",
    "            plt.xlim([-1000, 1000])\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            #print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            #print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0,(num_steps-1)/print_step+1), acc_valid, '.', color='b')\n",
    "plt.plot((-1)*np.arange(0,(num_steps-1)/print_step+1),acc_test, '.', color='g')\n",
    "plt.ylim([0.45, 0.75])\n",
    "plt.xlim([-1000, 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(acc_test, acc_valid, '.', color='b')\n",
    "#plt.plot((-1)*np.arange(0,(num_steps-1)/print_step+1),losses, '.', color='g')\n",
    "plt.ylim([0.4, 0.7])\n",
    "plt.xlim([0.0, 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Open questions:**\n",
    "+ Why is there so much scatter in the loss function over time?\n",
    "+ Is there structure in the loss function over time?\n",
    "+ If I plot loss vs. accuracy, what do I get? \n",
    "+ Do I really see a difference when I scramble vs. leave in order, and if so, is it because of the way SGD interacts with the two cosmologies?\n",
    "+ will deeper / better networks get us over 65%?\n",
    "+ Why, oh why, are my test and valid data sets so damn well correlated??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph.get_tensor_by_name.im_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KedKkn4EutIK"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klf21gpbAgb-"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
